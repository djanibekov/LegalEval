{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jonibekmansurov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, XLNetTokenizerFast\n",
    "\n",
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Digits, Whitespace, Punctuation\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              Articles\n0    1. Name and territory of the Union\\n(1) India,...\n1    1. The territories of the States; the Union te...\n2    2. Admission or establishment of new States: P...\n3    2A. Sikkim to be associated with the Union Rep...\n4    3. Formation of new States and alteration of a...\n..                                                 ...\n451  378A. Special provision as to duration of Andh...\n452  392. Power of the President to remove difficul...\n453  393. Short title This Constitution may be call...\n454  394. Commencement This article and Articles 5,...\n455  395. Repeals The Indian Independence Act, 1947...\n\n[456 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Articles</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1. Name and territory of the Union\\n(1) India,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1. The territories of the States; the Union te...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2. Admission or establishment of new States: P...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2A. Sikkim to be associated with the Union Rep...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3. Formation of new States and alteration of a...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>451</th>\n      <td>378A. Special provision as to duration of Andh...</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>392. Power of the President to remove difficul...</td>\n    </tr>\n    <tr>\n      <th>453</th>\n      <td>393. Short title This Constitution may be call...</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>394. Commencement This article and Articles 5,...</td>\n    </tr>\n    <tr>\n      <th>455</th>\n      <td>395. Repeals The Indian Independence Act, 1947...</td>\n    </tr>\n  </tbody>\n</table>\n<p>456 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constitution = pd.read_csv('data/external/constitution_of_india.csv')\n",
    "constitution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         processed_text  \\\n0      F NARIMAN J Leave granted In 2008 the Punjab ...   \n1      S THAKUR J Leave granted These appeals are di...   \n2      Markandey Katju J Leave granted Heard learned...   \n3      ALTAMAS KABIRJ Leave granted The question whe...   \n4      CIVIL APPEAL NO 598 OF 2007 K MATHUR J This a...   \n...                                                 ...   \n6071  civil appellate jurisdiction civil appeal numb...   \n6072  criminal appellate jurisdiction special leave\\...   \n6073  civil appellate jurisdiction civil appeal numb...   \n6074  civil appellate jurisdiction civil appeal numb...   \n6075  criminal appellate jurisdiction criminal appea...   \n\n                                   constitution_article  \\\n0     ~~Equality before law The State shall not deny...   \n1     ~~Power of Parliament to amend the Constitutio...   \n2                                               [NOArt]   \n3     ~~Power of High Courts to issue certain writs1...   \n4     ~~Jurisdiction of existing High Courts Subject...   \n...                                                 ...   \n6071                                            [NOArt]   \n6072                                            [NOArt]   \n6073                                            [NOArt]   \n6074                                            [NOArt]   \n6075                                            [NOArt]   \n\n                                           crpc_article  \\\n0                                               [NOArt]   \n1                                               [NOArt]   \n2     ~~Order for maintenance of wives children and ...   \n3     ~~Compounding of offences The offences punisha...   \n4                                               [NOArt]   \n...                                                 ...   \n6071                                            [NOArt]   \n6072                                            [NOArt]   \n6073                                            [NOArt]   \n6074                                            [NOArt]   \n6075                                            [NOArt]   \n\n                                            ipc_article  label  split  \n0                                               [NOArt]      1  train  \n1                                               [NOArt]      0  train  \n2                                               [NOArt]      1  train  \n3     ~~Whoever commits murder shall be punished wit...      1  train  \n4                                               [NOArt]      1  train  \n...                                                 ...    ...    ...  \n6071                                            [NOArt]      1    dev  \n6072                                            [NOArt]      0    dev  \n6073                                            [NOArt]      0    dev  \n6074                                            [NOArt]      1    dev  \n6075                                            [NOArt]      0    dev  \n\n[6076 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>processed_text</th>\n      <th>constitution_article</th>\n      <th>crpc_article</th>\n      <th>ipc_article</th>\n      <th>label</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>F NARIMAN J Leave granted In 2008 the Punjab ...</td>\n      <td>~~Equality before law The State shall not deny...</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>1</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>S THAKUR J Leave granted These appeals are di...</td>\n      <td>~~Power of Parliament to amend the Constitutio...</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Markandey Katju J Leave granted Heard learned...</td>\n      <td>[NOArt]</td>\n      <td>~~Order for maintenance of wives children and ...</td>\n      <td>[NOArt]</td>\n      <td>1</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ALTAMAS KABIRJ Leave granted The question whe...</td>\n      <td>~~Power of High Courts to issue certain writs1...</td>\n      <td>~~Compounding of offences The offences punisha...</td>\n      <td>~~Whoever commits murder shall be punished wit...</td>\n      <td>1</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CIVIL APPEAL NO 598 OF 2007 K MATHUR J This a...</td>\n      <td>~~Jurisdiction of existing High Courts Subject...</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>1</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6071</th>\n      <td>civil appellate jurisdiction civil appeal numb...</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>1</td>\n      <td>dev</td>\n    </tr>\n    <tr>\n      <th>6072</th>\n      <td>criminal appellate jurisdiction special leave\\...</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>0</td>\n      <td>dev</td>\n    </tr>\n    <tr>\n      <th>6073</th>\n      <td>civil appellate jurisdiction civil appeal numb...</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>0</td>\n      <td>dev</td>\n    </tr>\n    <tr>\n      <th>6074</th>\n      <td>civil appellate jurisdiction civil appeal numb...</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>1</td>\n      <td>dev</td>\n    </tr>\n    <tr>\n      <th>6075</th>\n      <td>criminal appellate jurisdiction criminal appea...</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>[NOArt]</td>\n      <td>0</td>\n      <td>dev</td>\n    </tr>\n  </tbody>\n</table>\n<p>6076 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/final_data/processed_data.csv')\n",
    "data.fillna('[NOArt]', inplace=True)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# X_train_dev, X_test, y_train_dev, y_test = train_test_split(data.drop(columns=['label']),\n",
    "#                                                     data['label'], test_size=0.2, random_state=10)\n",
    "#\n",
    "# X_train, X_dev, y_train, y_dev = train_test_split(X_train_dev,\n",
    "#                                                   y_train_dev, test_size=0.1, random_state=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train = data.loc[data.split == 'train']\n",
    "dev = data.loc[data.split == 'dev']\n",
    "\n",
    "X_train = train.drop(columns=['label'])\n",
    "X_dev = dev.drop(columns=['label'])\n",
    "y_train = train.label\n",
    "y_dev = dev.label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_dev = X_dev.reset_index(drop=True)\n",
    "# X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_dev = y_dev.reset_index(drop=True)\n",
    "# y_test = y_test.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# train a tokenizer, initialize WordLevel tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "# we first define a normalizer applied before tokenization\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "# pre-tokenizer defines a \"preprocessing\" before the tokenization.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(),\n",
    "                                                   Digits(individual_digits=True)])\n",
    "# training a tokenizer is effectively building a vocabulary in this case\n",
    "trainer = WordLevelTrainer(vocab_size=50000, special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "tokenizer.train_from_iterator(X_train.processed_text.values, trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "#load a tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\"\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "class LegEval(Dataset):\n",
    "    def __init__(self, legal_docs, constitutions, CrPcs, IPCs, labels, tokenizer, max_token_len=512):\n",
    "        self.legal_docs = legal_docs\n",
    "        self.constitutions = constitutions\n",
    "        self.CrPcs = CrPcs\n",
    "        self.IPCs = IPCs\n",
    "\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.legal_docs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        legal_doc = self.legal_docs[idx]\n",
    "\n",
    "        constitution = self.constitutions[idx]\n",
    "        CrPc = self.CrPcs[idx]\n",
    "        IPC = self.IPCs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode(\n",
    "            legal_doc,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_token_len,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        constitution_encoding = self.tokenizer.encode(\n",
    "            constitution,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_token_len,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        CrPc_encoding = self.tokenizer.encode(\n",
    "            CrPc,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_token_len,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        IPC_encoding = self.tokenizer.encode(\n",
    "            IPC,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_token_len,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            input_ids=encoding,\n",
    "            input_constitution=constitution_encoding,\n",
    "            input_CrPc=CrPc_encoding,\n",
    "            input_IPC=IPC_encoding,\n",
    "            label=torch.tensor([label], dtype=torch.float),\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_dataset = LegEval(\n",
    "    X_train.processed_text, X_train.constitution_article,\n",
    "    X_train.crpc_article, X_train.ipc_article,\n",
    "    y_train, tokenizer\n",
    ")\n",
    "\n",
    "# test_dataset = LegEval(\n",
    "#     X_test.processed_text, X_test.constitution_article,\n",
    "#     X_test.crpc_article, X_test.ipc_article,\n",
    "#     y_test, tokenizer\n",
    "# )\n",
    "\n",
    "dev_dataset = LegEval(\n",
    "    X_dev.processed_text, X_dev.constitution_article,\n",
    "    X_dev.crpc_article, X_dev.ipc_article,\n",
    "    y_dev, tokenizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)AXC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class CNNClassifier_1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 output_size,\n",
    "                 embedding_size=300,\n",
    "                 in_channels=1,\n",
    "                 out_channels=100,\n",
    "                 kernel_sizes=[3,4,5]):\n",
    "        super(CNNClassifier_1, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(self.in_channels, self.out_channels,\n",
    "                       (kernel_size, self.embedding_size))\n",
    "             for kernel_size in self.kernel_sizes])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(self.kernel_sizes) * self.out_channels,\n",
    "                             self.output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(torch.squeeze(x))  # (batch_size, sequence_length, embedding_size)\n",
    "        x = x.unsqueeze(1)  # (batch_size, in_channels, sequence_length, embedding_size)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, out_channels, embedding_size), ...]*len(kernel_sizes)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(batch_size, out_channels), ...]*len(kernel_sizes)\n",
    "        x = torch.cat(x, 1)  # (batch_size, len(kernel_sizes)*out_channels)\n",
    "        x = self.dropout(x)  # (batch_size, len(kernel_sizes)*out_channels)\n",
    "        y = self.fc1(x)  # (batch_size, output_size)\n",
    "        return y\n",
    "\n",
    "    def predict(self, x, threshold=0.5):\n",
    "        preds = self.sigmoid(self.forward(x))\n",
    "        preds = np.array(preds.cpu() > threshold, dtype=float)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class CNNClassifier_2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 output_size,\n",
    "                 embedding_size=300,\n",
    "                 in_channels=1,\n",
    "                 out_channels=100,\n",
    "                 kernel_sizes=[3,4,5]):\n",
    "        super(CNNClassifier_2, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.output2=50\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.input2 = nn.Linear(embedding_size, self.output2)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(self.in_channels, self.out_channels,\n",
    "                       (kernel_size, self.embedding_size))\n",
    "             for kernel_size in self.kernel_sizes])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(self.kernel_sizes) * self.out_channels+self.output2,\n",
    "                             self.output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = self.embed(torch.squeeze(x))  # (batch_size, sequence_length, embedding_size)\n",
    "        # print(labels)\n",
    "        l = self.embed(torch.squeeze(labels))\n",
    "        input2 = self.input2(l)\n",
    "        x = x.unsqueeze(1)  # (batch_size, in_channels, sequence_length, embedding_size)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, out_channels, embedding_size), ...]*len(kernel_sizes)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(batch_size, out_channels), ...]*len(kernel_sizes)\n",
    "        x = torch.cat(x, 1)  # (batch_size, len(kernel_sizes)*out_channels)\n",
    "        x = self.dropout(x)  # (batch_size, len(kernel_sizes)*out_channels)\n",
    "        combined = torch.cat((x.view(x.size(0), -1),\n",
    "                              input2.view(input2.size(0), -1)), dim=1)\n",
    "        y = self.fc1(combined)  # (batch_size, output_size)\n",
    "        return y\n",
    "\n",
    "    def predict(self, x, labels, threshold=0.5):\n",
    "        preds = self.sigmoid(self.forward(x, labels))\n",
    "        preds = np.array(preds.cpu() > threshold, dtype=float)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class RnnType:\n",
    "    GRU = 1\n",
    "    LSTM = 2\n",
    "\n",
    "class AttentionModel:\n",
    "    NONE = 0\n",
    "    DOT = 1\n",
    "    GENERAL = 2\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, data_dict):\n",
    "        for k, v in data_dict.items():\n",
    "            exec(\"self.%s=%s\" % (k, v))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, device, method, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "\n",
    "        if self.method == AttentionModel.GENERAL:\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, rnn_outputs, final_hidden_state):\n",
    "        # rnn_output.shape:         (batch_size, seq_len, hidden_size)\n",
    "        # final_hidden_state.shape: (batch_size, hidden_size)\n",
    "        # NOTE: hidden_size may also reflect bidirectional hidden states (hidden_size = num_directions * hidden_dim)\n",
    "        batch_size, seq_len, _ = rnn_outputs.shape\n",
    "        if self.method == AttentionModel.DOT:\n",
    "            attn_weights = torch.bmm(rnn_outputs, final_hidden_state.unsqueeze(2))\n",
    "        elif self.method == AttentionModel.GENERAL:\n",
    "            attn_weights = self.attn(rnn_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "            attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"[Error] Unknown AttentionModel.\")\n",
    "\n",
    "        attn_weights = torch.softmax(attn_weights.squeeze(2), dim=1)\n",
    "\n",
    "        context = torch.bmm(rnn_outputs.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))\n",
    "\n",
    "        return attn_hidden, attn_weights\n",
    "\n",
    "\n",
    "class RnnClassifier(nn.Module):\n",
    "    def __init__(self, device, params):\n",
    "        super(RnnClassifier, self).__init__()\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding layer\n",
    "        self.word_embeddings = nn.Embedding(self.params.vocab_size, self.params.embed_dim)\n",
    "\n",
    "        # Calculate number of directions\n",
    "        self.num_directions = 2 if self.params.bidirectional == True else 1\n",
    "\n",
    "        self.linear_dims = [self.params.rnn_hidden_dim * self.num_directions] + self.params.linear_dims\n",
    "        self.linear_dims.append(self.params.label_size)\n",
    "\n",
    "        # RNN layer\n",
    "        rnn = None\n",
    "        if self.params.rnn_type == RnnType.GRU:\n",
    "            rnn = nn.GRU\n",
    "        elif self.params.rnn_type == RnnType.LSTM:\n",
    "            rnn = nn.LSTM\n",
    "        else:\n",
    "            raise Exception(\"[Error] Unknown RnnType. Currently supported: RnnType.GRU=1, RnnType.LSTM=2\")\n",
    "        self.rnn = rnn(self.params.embed_dim,\n",
    "                       self.params.rnn_hidden_dim,\n",
    "                       num_layers=self.params.num_layers,\n",
    "                       bidirectional=self.params.bidirectional,\n",
    "                       dropout=self.params.dropout,\n",
    "                       batch_first=False)\n",
    "\n",
    "\n",
    "        # Define set of fully connected layers (Linear Layer + Activation Layer) * #layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(0, len(self.linear_dims)-1):\n",
    "            if self.params.dropout > 0.0:\n",
    "                self.linears.append(nn.Dropout(p=self.params.dropout))\n",
    "            linear_layer = nn.Linear(self.linear_dims[i], self.linear_dims[i+1])\n",
    "            self.init_weights(linear_layer)\n",
    "            self.linears.append(linear_layer)\n",
    "            if i == len(self.linear_dims) - 1:\n",
    "                break  # no activation after output layer!!!\n",
    "            self.linears.append(nn.ReLU())\n",
    "\n",
    "        self.hidden = None\n",
    "\n",
    "        # Choose attention model\n",
    "        if self.params.attention_model != AttentionModel.NONE:\n",
    "            self.attn = Attention(self.device, self.params.attention_model, self.params.rnn_hidden_dim * self.num_directions)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.params.rnn_type == RnnType.GRU:\n",
    "            return torch.zeros(self.params.num_layers * self.num_directions, batch_size, self.params.rnn_hidden_dim).to(self.device)\n",
    "        elif self.params.rnn_type == RnnType.LSTM:\n",
    "            return (torch.zeros(self.params.num_layers * self.num_directions, batch_size, self.params.rnn_hidden_dim).to(self.device),\n",
    "                    torch.zeros(self.params.num_layers * self.num_directions, batch_size, self.params.rnn_hidden_dim).to(self.device))\n",
    "        else:\n",
    "            raise Exception('Unknown rnn_type. Valid options: \"gru\", \"lstm\"')\n",
    "\n",
    "    # def freeze_layer(self, layer):\n",
    "    #     for param in layer.parameters():\n",
    "    #         param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, ems = inputs.shape\n",
    "\n",
    "        # Push through embedding layer\n",
    "        X = self.word_embeddings(torch.squeeze(inputs)).transpose(0, 1)\n",
    "\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        # Push through RNN layer\n",
    "        rnn_output, self.hidden = self.rnn(X, self.hidden)\n",
    "\n",
    "        # Extract last hidden state\n",
    "        final_state = None\n",
    "        if self.params.rnn_type == RnnType.GRU:\n",
    "            final_state = self.hidden.view(self.params.num_layers, self.num_directions, batch_size, self.params.rnn_hidden_dim)[-1]\n",
    "        elif self.params.rnn_type == RnnType.LSTM:\n",
    "            final_state = self.hidden[0].view(self.params.num_layers, self.num_directions, batch_size, self.params.rnn_hidden_dim)[-1]\n",
    "        # Handle directions\n",
    "        final_hidden_state = None\n",
    "        if self.num_directions == 1:\n",
    "            final_hidden_state = final_state.squeeze(0)\n",
    "        elif self.num_directions == 2:\n",
    "            h_1, h_2 = final_state[0], final_state[1]\n",
    "            final_hidden_state = torch.cat((h_1, h_2), 1)  # Concatenate both states\n",
    "\n",
    "        # Push through attention layer\n",
    "        if self.params.attention_model != AttentionModel.NONE:\n",
    "            rnn_output = rnn_output.permute(1, 0, 2)  #\n",
    "            X = self.attn(rnn_output, final_hidden_state)[0]\n",
    "        else:\n",
    "            X = final_hidden_state\n",
    "\n",
    "        # Push through linear layers\n",
    "        for l in self.linears:\n",
    "            X = l(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            # print(\"Initialize layer with nn.init.xavier_uniform_: {}\".format(layer))\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(0.01)\n",
    "\n",
    "    def predict(self, x, threshold=0.5):\n",
    "        preds = self.sigmoid(self.forward(x))\n",
    "        # print(preds)\n",
    "        preds = np.array(preds.cpu() > threshold, dtype=float)\n",
    "        # print(preds)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss >= (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "parameters_dictionary = {}\n",
    "parameters = Parameters({'vocab_size': tokenizer.vocab_size, 'embed_dim': 200,\n",
    "                         'rnn_hidden_dim': 300, 'bidirectional': True, 'linear_dims': [300, 1],\n",
    "                         'label_size': 1, 'rnn_type': RnnType.GRU, 'num_layers': 1,\n",
    "                         'dropout': 0.0, 'attention_model': AttentionModel.GENERAL}\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-4bd324f69be2>:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  dev_loss = loss_fun(torch.FloatTensor(outputs), torch.FloatTensor(targets))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30, Micro-f1: 0.555, Train Loss: 0.044, Dev Loss: 0.718"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30 # epoch\n",
    "LR = 0.01  # learning rate\n",
    "\n",
    "# model = CNNClassifier_1(\n",
    "#     tokenizer.vocab_size,\n",
    "#     1\n",
    "# )\n",
    "\n",
    "model = RnnClassifier(\n",
    "    torch.device(device),\n",
    "    parameters\n",
    ")\n",
    "\n",
    "# model = CNNClassifier_2(\n",
    "#     tokenizer.vocab_size,\n",
    "#     2\n",
    "# )\n",
    "model.to(device)\n",
    "\n",
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0.02)\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # print(data['label'])\n",
    "        # print(idx)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data['input_ids'].to(device))\n",
    "        # print(outputs)\n",
    "        loss = loss_fun(outputs, data['label'].to(device))\n",
    "        loss.backward()\n",
    "        # print(model.linears[4].weight.grad)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dev_dataloader):\n",
    "\n",
    "            output_batch = model.predict(data['input_ids'].to(device))\n",
    "            target_batch = np.array(data['label'])\n",
    "            outputs.extend(output_batch)\n",
    "            targets.extend(target_batch)\n",
    "            # dev_dataloader[idx]['label_1_output'] = outputs\n",
    "\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    dev_loss = loss_fun(torch.FloatTensor(outputs), torch.FloatTensor(targets))\n",
    "    if early_stopper.early_stop(dev_loss):\n",
    "        break\n",
    "    print(f'\\rEpoch: {epoch}/{EPOCHS}, Micro-f1: {micro_f1:.3f}, Train Loss: {epoch_loss/len(train_dataloader):.3f}, Dev Loss: {dev_loss:.3f}', end='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.58      0.55       497\n",
      "         1.0       0.54      0.49      0.51       497\n",
      "\n",
      "    accuracy                           0.53       994\n",
      "   macro avg       0.53      0.53      0.53       994\n",
      "weighted avg       0.53      0.53      0.53       994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets, outputs))\n",
    "\n",
    "dev_dataset = LegEval(\n",
    "    X_dev.processed_text, X_dev.constitution_article,\n",
    "    X_dev.crpc_article, X_dev.ipc_article,\n",
    "    y_dev, tokenizer\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_dataset = LegEval(\n",
    "#     X_train.processed_text, X_train.constitution_article,\n",
    "#     X_train.crpc_article, X_train.ipc_article,\n",
    "#     y_train, tokenizer\n",
    "# )\n",
    "#\n",
    "# test_dataset = LegEval(\n",
    "#     X_test.processed_text, X_test.constitution_article,\n",
    "#     X_test.crpc_article, X_test.ipc_article,\n",
    "#     y_test, tokenizer\n",
    "# )\n",
    "#\n",
    "# dev_dataset = LegEval(\n",
    "#     X_dev.processed_text, X_dev.constitution_article,\n",
    "#     X_dev.crpc_article, X_dev.ipc_article,\n",
    "#     y_dev, tokenizer\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "train_outputs = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        output_batch = model.predict(data['input_ids'].to(device))\n",
    "        target_batch = np.array(data['label'])\n",
    "        train_outputs.extend(output_batch)\n",
    "        targets.extend(target_batch)\n",
    "\n",
    "print(classification_report(targets, train_outputs))\n",
    "\n",
    "train_dataset = LegEval(\n",
    "    X_train.processed_text, X_train.constitution_article,\n",
    "    X_train.crpc_article, X_train.ipc_article,\n",
    "    y_train, tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# outputs_test = []\n",
    "# targets = []\n",
    "# with torch.no_grad():\n",
    "#     for idx, data in enumerate(test_dataloader):\n",
    "#         output_batch = model.predict(data['input_ids'].to(device))\n",
    "#         target_batch = np.array(data['label'])\n",
    "#         outputs_test.extend(output_batch)\n",
    "#         targets.extend(target_batch)\n",
    "#\n",
    "# print(classification_report(targets, outputs_test))\n",
    "#\n",
    "# test_dataset = LegEval(\n",
    "#     X_test.processed_text, X_test.constitution_article,\n",
    "#     X_test.crpc_article, X_test.ipc_article,\n",
    "#     y_test, tokenizer\n",
    "# )\n",
    "#\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30 # epoch\n",
    "LR = 0.001  # learning rate\n",
    "\n",
    "model = CNNClassifier_2(\n",
    "    tokenizer.vocab_size,\n",
    "    2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0.02)\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        # print(data['label'])\n",
    "        # print(idx)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data['input_ids'].to(device), data['input_constitution'])\n",
    "        # print(outputs)\n",
    "        loss = loss_fun(outputs, data['label'].to(device))\n",
    "        loss.backward()\n",
    "        # print(model.linears[4].weight.grad)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dev_dataloader):\n",
    "\n",
    "            output_batch = model.predict(data['input_ids'].to(device), data['input_constitution'])\n",
    "            target_batch = np.array(data['label'])\n",
    "            outputs.extend(output_batch)\n",
    "            targets.extend(target_batch)\n",
    "            # dev_dataloader[idx]['label_1_output'] = outputs\n",
    "\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    dev_loss = loss_fun(torch.FloatTensor(outputs), torch.FloatTensor(targets))\n",
    "    if early_stopper.early_stop(dev_loss):\n",
    "        break\n",
    "    print(f'\\rEpoch: {epoch}/{EPOCHS}, Micro-f1: {micro_f1:.3f}, Train Loss: {epoch_loss/len(train_dataloader):.3f}, Dev Loss: {dev_loss:.3f}', end='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.has_mps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}